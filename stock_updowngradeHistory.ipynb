{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import pandas_datareader as pdr\n",
    "from urllib import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "import csv\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "from time import sleep\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.tseries.offsets import BDay\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "\n",
    "no_hist_symbol = []\n",
    "DIR = 'C:/Users/sarab/Desktop/git_projects/stock_analyst_grade_history/'\n",
    "URL = 'https://finance.yahoo.com/quote/{0}/analysts?p={0}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#### Populating Ticker Lists #################################################\n",
    "##############################################################################\n",
    "\n",
    "def locate_all_symbols(suffix,base_filename,dir_name):\n",
    "    script_dir = os.path.join(dir_name,'.'.join((base_filename,suffix))) \n",
    "    all_companies = pd.read_csv(script_dir)\n",
    "    all_ticker_symbols = all_companies['Symbol'].tolist()\n",
    "    return all_ticker_symbols\n",
    "\n",
    "def save_sp500_tickers():\n",
    "    resp = requests.get('http://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "    soup = BeautifulSoup(resp.text, 'lxml')\n",
    "    table = soup.find('table', {'class': 'wikitable sortable'})\n",
    "    tickers = []\n",
    "    for row in table.findAll('tr')[1:]:\n",
    "        ticker = row.findAll('td')[0].text\n",
    "        tickers.append(ticker)      \n",
    "    with open(\"sp500tickers.pickle\",\"wb\") as f:\n",
    "        pickle.dump(tickers,f)    \n",
    "    return tickers    \n",
    "\n",
    "\n",
    "entire_ticker_list = locate_all_symbols('csv','company_ticker_symbols', DIR)\n",
    "snp500tickers = save_sp500_tickers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_ticker(tick):\n",
    "    url = 'https://finance.yahoo.com/quote/{0}/analysts?p={0}'\n",
    "    return url.format(tick)\n",
    "\n",
    "def yahoo_finance_analyst_data(url):\n",
    "    req = urlopen(url)\n",
    "    raw = req.read()\n",
    "    soup = BeautifulSoup(raw,'lxml')\n",
    "    return soup\n",
    "\n",
    "def auto_symbols(ticker):\n",
    "    url = select_ticker(ticker)\n",
    "    return url\n",
    "\n",
    "def grab_dict(soup):\n",
    "    l = [i for i in soup.find_all('script')]\n",
    "    mylist = [str(item) for item in l]\n",
    "    m = max(mylist,key=len)\n",
    "    ind = mylist.index(m)\n",
    "    data = l[mylist.index(m)]\n",
    "    data_dict = data.text[112:-12]\n",
    "    all_tables = json.loads(data_dict)\n",
    "    return all_tables\n",
    "\n",
    "def upgrade_downgrade_pandas(dic,tick):\n",
    "    try:\n",
    "        stock_upgrade_downgrade_history = dic['context']['dispatcher']['stores']['QuoteSummaryStore']['upgradeDowngradeHistory']['history']\n",
    "        pd_stock_upgrade_downgrade_history = pd.DataFrame(stock_upgrade_downgrade_history)\n",
    "        pd_stock_upgrade_downgrade_history['epochGradeDate'] = pd.to_datetime(pd_stock_upgrade_downgrade_history['epochGradeDate'],unit='s')\n",
    "        pd_stock_upgrade_downgrade_history.columns.values[1] = 'date'\n",
    "        pd_stock_upgrade_downgrade_history.insert(0,'stockTicker',tick)\n",
    "        return pd_stock_upgrade_downgrade_history\n",
    "    except Exception:\n",
    "        print \"error\" + tick\n",
    "        no_hist_symbol.append(tick)\n",
    "        \n",
    "\n",
    "def single_company_hist(tick):\n",
    "    url = select_ticker(tick)\n",
    "    soup = yahoo_finance_analyst_data(url)\n",
    "    full_dict = grab_dict(soup)\n",
    "    return upgrade_downgrade_pandas(full_dict,tick)\n",
    "\n",
    "def list_of_all_stock_grading_history(good_list):\n",
    "    \n",
    "    all_stock_grading_history = []\n",
    "    for ticker in good_list:\n",
    "        if ticker not in no_hist_symbol:\n",
    "            url = auto_symbols(ticker)\n",
    "            soup = yahoo_finance_analyst_data(url)\n",
    "            all_tables = grab_dict(soup)\n",
    "            grading_history = upgrade_downgrade_pandas(all_tables,ticker)\n",
    "            all_stock_grading_history.append(grading_history)\n",
    "    concat_lists_to_pd = pd.concat(all_stock_grading_history)\n",
    "    return concat_lists_to_pd\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "##### Trading Data   #########################################################\n",
    "##############################################################################\n",
    "\n",
    "def get_stock_history(df):\n",
    "    list_of_stock_histories = []\n",
    "    for ticker in df['stockTicker'].unique():\n",
    "        try:\n",
    "            stock_df = pdr.get_data_yahoo(symbols=ticker, start=df.loc[df['stockTicker']==ticker].date.iloc[-1])\n",
    "            stock_df['stockTicker'] = ticker\n",
    "            stock_df.reset_index(inplace = True)\n",
    "            stock_df['Percent_change'] =  stock_df.Close.pct_change()\n",
    "            list_of_stock_histories.append(stock_df)\n",
    "            print ticker\n",
    "        except Exception: \n",
    "            print 'error @ ' + ticker\n",
    "    concat_lists_to_pd = pd.concat(list_of_stock_histories)\n",
    "    return concat_lists_to_pd\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def business_days_and_datetimes(df1):\n",
    "    df1['Date'] = pd.to_datetime(df1['Date'])\n",
    "    df1['1_day_before_grade_Date'] = df1.loc[~df1['firm'].isnull()].Date.apply(lambda x: x + BDay(-1))\n",
    "    df1['1_day_from_grade_Date'] = df1.loc[~df1['firm'].isnull()].Date.apply(lambda x: x + BDay(1))\n",
    "    df1['30_days_from_grade_Date'] = df1.loc[~df1['firm'].isnull()].Date.apply(lambda x: x + BDay(22))\n",
    "    df1['60_days_from_grade_Date'] = df1.loc[~df1['firm'].isnull()].Date.apply(lambda x: x + BDay(44))\n",
    "    df1['90_days_from_grade_Date'] = df1.loc[~df1['firm'].isnull()].Date.apply(lambda x: x + BDay(66))\n",
    "\n",
    "    df1['1_day_before_grade_Date'] =  pd.to_datetime(df1['1_day_before_grade_Date'])\n",
    "    df1['1_day_from_grade_Date'] =  pd.to_datetime(df1['1_day_from_grade_Date'])\n",
    "    df1['30_days_from_grade_Date'] =  pd.to_datetime(df1['30_days_from_grade_Date'])\n",
    "    df1['60_days_from_grade_Date'] =  pd.to_datetime(df1['60_days_from_grade_Date'])\n",
    "    df1['90_days_from_grade_Date'] =  pd.to_datetime(df1['90_days_from_grade_Date'])\n",
    "    \n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_day_prices(df1):\n",
    "    lst=[]\n",
    "    unique_list = df1['stockTicker'].unique().tolist()\n",
    "    list_of_dfs = [df1.loc[df1.stockTicker==word] for word in unique_list]\n",
    "    for x in range(len(list_of_dfs)):\n",
    "        df = list_of_dfs[x]\n",
    "        \n",
    "        tick = df5['stockTicker'].unique().tolist()[x]\n",
    "\n",
    "    \n",
    "        alldaysfrom = df.loc[df['Date'].isin(df['1_day_from_grade_Date'])]['Date']\n",
    "        alldaysbefore = df.loc[df['Date'].isin(df['1_day_before_grade_Date'])]['Date']\n",
    "        day30 = df.loc[df['Date'].isin(df['30_days_from_grade_Date'])]['Date']\n",
    "        day60 = df.loc[df['Date'].isin(df['60_days_from_grade_Date'])]['Date']\n",
    "        day90 = df.loc[df['Date'].isin(df['90_days_from_grade_Date'])]['Date']\n",
    "        \n",
    "        pricesfrom = df.loc[df['Date'].isin(df['1_day_from_grade_Date'])].Open\n",
    "        pricesbefore = df.loc[df['Date'].isin(df['1_day_before_grade_Date'])].Open\n",
    "        prices30 = df.loc[df['Date'].isin(df['30_days_from_grade_Date'])].Open\n",
    "        prices60 = df.loc[df['Date'].isin(df['60_days_from_grade_Date'])].Open\n",
    "        prices90 = df.loc[df['Date'].isin(df['90_days_from_grade_Date'])].Open\n",
    "\n",
    "        allmerge = pd.DataFrame({'stockTicker':tick,\n",
    "                                 '1_day_from_grade_Date':alldaysfrom,\n",
    "                                 '1_day_from_grade_Date_Price':pricesfrom,\n",
    "                                 '1_day_before_grade_Date':alldaysbefore,\n",
    "                                 '1_day_before_grade_Date_Price':pricesbefore,\n",
    "                                 '30_days_from_grade_Date':day30,\n",
    "                                 '30_days_from_grade_Date_Price':prices30,\n",
    "                                 '60_days_from_grade_Date':day60,\n",
    "                                 '60_days_from_grade_Date_Price':prices60,\n",
    "                                 '90_days_from_grade_Date':day90,\n",
    "                                 '90_days_from_grade_Date_Price':prices90})\n",
    "        \n",
    "        lst.append(allmerge)\n",
    "    concat = pd.concat(lst)\n",
    "    day30sep = concat[['stockTicker','30_days_from_grade_Date','30_days_from_grade_Date_Price']]\n",
    "    day30sep = day30sep.loc[~day30sep['30_days_from_grade_Date'].isnull()].drop_duplicates(subset=['30_days_from_grade_Date','30_days_from_grade_Date_Price'])\n",
    "\n",
    "    day60sep = concat[['stockTicker','60_days_from_grade_Date','60_days_from_grade_Date_Price']]\n",
    "    day60sep = day60sep.loc[~day60sep['60_days_from_grade_Date'].isnull()].drop_duplicates(subset=['60_days_from_grade_Date','60_days_from_grade_Date_Price'])\n",
    "\n",
    "    day90sep = concat[['stockTicker','90_days_from_grade_Date','90_days_from_grade_Date_Price']]\n",
    "    day90sep = day90sep.loc[~day90sep['90_days_from_grade_Date'].isnull()].drop_duplicates(subset=['90_days_from_grade_Date','90_days_from_grade_Date_Price'])\n",
    "\n",
    "    day1bsep = concat[['stockTicker','1_day_before_grade_Date','1_day_before_grade_Date_Price']]\n",
    "    day1bsep = day1bsep.loc[~day1bsep['1_day_before_grade_Date'].isnull()].drop_duplicates(subset=['1_day_before_grade_Date','1_day_before_grade_Date_Price'])\n",
    "\n",
    "    day1asep = concat[['stockTicker','1_day_from_grade_Date','1_day_from_grade_Date_Price']]\n",
    "    day1asep = day1asep.loc[~day1asep['1_day_from_grade_Date'].isnull()].drop_duplicates(subset=['1_day_from_grade_Date','1_day_from_grade_Date_Price'])\n",
    "    \n",
    "    day30 = pd.merge(df1,day30sep,how='outer',on=['stockTicker','30_days_from_grade_Date'])\n",
    "    day6030 = pd.merge(day30,day60sep,how='outer',on=['stockTicker','60_days_from_grade_Date'])\n",
    "    day906030 = pd.merge(day6030,day90sep,how='outer',on=['stockTicker','90_days_from_grade_Date'])\n",
    "    day1906030 = pd.merge(day906030,day1bsep,how='outer', on=['stockTicker','1_day_before_grade_Date'])\n",
    "    with_from_before_price = pd.merge(day1906030,day1asep,how='outer', on=['stockTicker','1_day_from_grade_Date'])\n",
    "    \n",
    "    with_from_before_price['percent30'] = 100 * (with_from_before_price['30_days_from_grade_Date_Price'] - with_from_before_price['Open']) / with_from_before_price['Open']\n",
    "    with_from_before_price['percent60'] = 100 * (with_from_before_price['60_days_from_grade_Date_Price'] - with_from_before_price['Open']) / with_from_before_price['Open']\n",
    "    with_from_before_price['percent90'] = 100 * (with_from_before_price['90_days_from_grade_Date_Price'] - with_from_before_price['Open']) / with_from_before_price['Open']\n",
    "    with_from_before_price['percent1Before'] = 100 * (with_from_before_price['1_day_before_grade_Date_Price'] - with_from_before_price['Open']) / with_from_before_price['Open']\n",
    "    with_from_before_price['percent1After'] = 100 * (with_from_before_price['1_day_from_grade_Date_Price'] - with_from_before_price['Open']) / with_from_before_price['Open']\n",
    "\n",
    "    return with_from_before_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_toGrade(df1):\n",
    "    \n",
    "    Strong_Buy = ['Outperform','Mkt Outperformer','Recomm List','Market Outperform','Mkt Outperform','NT Strong Buy','Recommended List','LT Accumulate','Outperformer','Top Pick', 'Strong Buy','Outperf. Signif.','Sector Outperform','Conviction Buy']\n",
    "    Buy = ['Buy','Overweight','Trading Buy','NT Accum/LT Buy', 'NT Ntrl/LT Buy','Long-Term Buy','Above Average','LT Attractive','LT Accum','LT Buy','Accumulate','NT Accum','NT Nuet/ LT Buy','Attractive','Positive','Long-term Buy','Add','NT Buy']\n",
    "    Hold = ['Hold','Neutral','Peer perform','Equal-weight','Perform-In-Line','Perform In Line','In-line','Market Perform','Mkt Performer','Average','NT Neutral','Maintain Position','ST Neutral','Sector Perform','Mixed','Market Weight','Equal-weight','Perform','Sector Weight','Peer Perform','In-Line','Fair Value','Mkt Perform','Equal Weight']\n",
    "    Underperform = ['Underperform', 'Underperformer','Below Average','Unattractive','Market Underperform','Cautious','Mkt Underperform','Sector Underperform']\n",
    "    Sell = ['Sell','Underweight','Reduce','Negative','Trim']\n",
    "\n",
    "    list_dict = {\n",
    "    'Strong_buy':Strong_Buy,\n",
    "    'Buy':Buy,\n",
    "    'Hold':Hold,\n",
    "    'Underperform':Underperform,\n",
    "    'Sell':Sell\n",
    "    }\n",
    "    reversed_dict = {val: key for key in list_dict for val in list_dict[key]}\n",
    "    df1['toGrade_combined'] = df1['toGrade'].map(reversed_dict)\n",
    "    df1['fromGrade_combined'] = df1['fromGrade'].map(reversed_dict)\n",
    "    return df1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S&P 500 Dataset\n",
    "* don't uncomment and run, this code creates the database, the database is already created through the file snp500merged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* snp500data gathers stock ticker analyst upgrade downgrade history, and is written into a csv called 'snp500analyst_history'\n",
    "* snp500StockValueHistories takes the dates of all the stock data and returns the stock chart, and is written to the file 'snp500stockHistories'\n",
    "\n",
    "* sfinal merges both of the two dataframes on 'stockTicker' and 'Date' and this dataframe is written to a csv called 'snp500merged'\n",
    "    - Dates and Stock Prices are added 1 day before, 1 day after, 30 days after, 60 days after, and 90 days after\n",
    "    - Percent Changes are added between the Open Price and the days added above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# snp500data = list_of_all_stock_grading_history(snp500tickers)\n",
    "# snp500data.to_csv('snp500analyst_history')\n",
    "# snp500StockValueHistories = get_stock_history(snp_500_data_df)\n",
    "# snp500StockValueHistories.to_csv('snp500stockHistories')\n",
    "# snp_500_analyst_history_df = pd.read_csv('snp500analyst_history')\n",
    "# snp_500_stock_history_df = pd.read_csv('snp500stockHistories')\n",
    "# snp500_stock_and_updown_hist_merged_df = snp_500_stock_history_df.merge(snp_500_analyst_history_df,how='left',left_on= ['Date','stockTicker'], right_on=['date','stockTicker'])\n",
    "# cols = list(snp500_stock_and_updown_hist_merged_df.columns.values)\n",
    "# snp500_stock_and_updown_hist_merged_df = snp500_stock_and_updown_hist_merged_df.drop('Unnamed: 0_y',1)\n",
    "# snp500_stock_and_updown_hist_merged_df = snp500_stock_and_updown_hist_merged_df[['stockTicker','Date','firm','action','fromGrade','toGrade','Open', 'High','Low','Close','Adj Close','Volume']]\n",
    "# df5 = snp500_stock_and_updown_hist_merged_df.copy()\n",
    "# business_days = business_days_and_datetimes(df5)\n",
    "# with_from_before_price = merge_day_prices(list_of_dfs,business_days)\n",
    "# finaldf = combine_toGrade(with_from_before_price)\n",
    "# finaldf.to_csv('snp500merged')\n",
    "# finaldf.drop('Unnamed: 0',1)\n",
    "stock_analyst_df = pd.read_csv('snp500merged')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Removed day high, day low, and adj close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "finaldf = finaldf[['stockTicker',\n",
    " 'Date',\n",
    " 'Open',\n",
    " 'Close',\n",
    " 'Volume',\n",
    " 'firm',\n",
    " 'action',\n",
    " 'fromGrade',\n",
    " 'fromGrade_combined',          \n",
    " 'toGrade',\n",
    " 'toGrade_combined',\n",
    " '1_day_before_grade_Date',\n",
    " '1_day_before_grade_Date_Price',\n",
    " 'percent1Before',\n",
    " '1_day_from_grade_Date',\n",
    " '1_day_from_grade_Date_Price',\n",
    " 'percent1After',\n",
    " '30_days_from_grade_Date',\n",
    " '30_days_from_grade_Date_Price',\n",
    " 'percent30',\n",
    " '60_days_from_grade_Date',\n",
    " '60_days_from_grade_Date_Price',\n",
    " 'percent60',\n",
    " '90_days_from_grade_Date',\n",
    " '90_days_from_grade_Date_Price',\n",
    " 'percent90']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "finaldf.to_csv('snp500merged')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Stock Ticker Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#all_ticker_data = list_of_all_stock_grading_history(entire_ticker_list)\n",
    "#all_ticker_data.to_csv('entire_ticker_analyst_history')\n",
    "#all_ticker_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_ticker_data = pd.read_csv('C:\\Users\\sarab\\Desktop\\git_projects\\stock_analyst_grade_history\\entire_ticker_analyst_history')\n",
    "snp_500_data_df = pd.read_csv('C:\\Users\\sarab\\Desktop\\git_projects\\stock_analyst_grade_history\\snp500analyst_history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Company Lookup by Ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = single_company_hist(raw_input('Please Enter Ticker Value').upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>stockTicker</th>\n",
       "      <th>Percent_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-02-09</td>\n",
       "      <td>87.540001</td>\n",
       "      <td>88.180000</td>\n",
       "      <td>87.459999</td>\n",
       "      <td>88.019997</td>\n",
       "      <td>75.877464</td>\n",
       "      <td>3031600.0</td>\n",
       "      <td>MMM</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-02-10</td>\n",
       "      <td>87.199997</td>\n",
       "      <td>87.559998</td>\n",
       "      <td>86.750000</td>\n",
       "      <td>87.139999</td>\n",
       "      <td>75.118858</td>\n",
       "      <td>3085700.0</td>\n",
       "      <td>MMM</td>\n",
       "      <td>-0.009998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-02-13</td>\n",
       "      <td>87.570000</td>\n",
       "      <td>88.089996</td>\n",
       "      <td>87.160004</td>\n",
       "      <td>88.029999</td>\n",
       "      <td>75.886070</td>\n",
       "      <td>2530600.0</td>\n",
       "      <td>MMM</td>\n",
       "      <td>0.010213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-02-14</td>\n",
       "      <td>87.599998</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>87.220001</td>\n",
       "      <td>87.989998</td>\n",
       "      <td>75.851593</td>\n",
       "      <td>2850300.0</td>\n",
       "      <td>MMM</td>\n",
       "      <td>-0.000454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-02-15</td>\n",
       "      <td>87.419998</td>\n",
       "      <td>87.720001</td>\n",
       "      <td>86.750000</td>\n",
       "      <td>87.010002</td>\n",
       "      <td>75.513138</td>\n",
       "      <td>3647000.0</td>\n",
       "      <td>MMM</td>\n",
       "      <td>-0.011138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date       Open       High        Low      Close  Adj Close  \\\n",
       "0 2012-02-09  87.540001  88.180000  87.459999  88.019997  75.877464   \n",
       "1 2012-02-10  87.199997  87.559998  86.750000  87.139999  75.118858   \n",
       "2 2012-02-13  87.570000  88.089996  87.160004  88.029999  75.886070   \n",
       "3 2012-02-14  87.599998  88.000000  87.220001  87.989998  75.851593   \n",
       "4 2012-02-15  87.419998  87.720001  86.750000  87.010002  75.513138   \n",
       "\n",
       "      Volume stockTicker  Percent_change  \n",
       "0  3031600.0         MMM             NaN  \n",
       "1  3085700.0         MMM       -0.009998  \n",
       "2  2530600.0         MMM        0.010213  \n",
       "3  2850300.0         MMM       -0.000454  \n",
       "4  3647000.0         MMM       -0.011138  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snp500StockValueHistories.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SNP500 Stock Value Histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#snp500StockValueHistories = get_stock_history(snp_500_data_df)\n",
    "#snp500StockValueHistories.to_csv('snp500stockHistories')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock History and Analyst History Merged Data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
